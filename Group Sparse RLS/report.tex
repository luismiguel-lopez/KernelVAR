
%\documentclass[journal]{IEEEtran}
%\documentclass[draftclsnofoot,onecolumn,12pt]{IEEEtran}
\documentclass[11pt,final,onecolumn]{IEEEtran}
%\documentclass[10pt,final,twocolumn]{IEEEtran}
%\documentclass[twocolumn,twoside]{IEEEtran}



%%%%%%%%%%%%%% SETTINGS %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% SET FOLLOWING VARIABLE TO 1 FOR EDIT MODE AND 0 FOR VIEW MODE
% (erase .bbl and .aux files in this folder every time
% you switch from one mode to another, otherwise you get
% an error)
% One of the features of the edit mode is that it shows citations in
% boldface if they are not defined in the bibtex file, like the [?] in
% view mode. Different from the view mode, the edit mode indicates
% which is the missing reference.
\def\editmode{1}

% set the following variable to the filenames, separated by commas, of
% the bibfiles that contain your references.  
\def\bibfilenames{WISENET}

% Use the following lines if you want to modify how citations are displayed
%% \let\oldcite\cite
%% \renewcommand{\cite}[1]{\textbf{\oldcite{#1}}}


%%%%%%%%%%%%%% BEGIN: DO NOT TOUCH REGION %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\if\editmode1  %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% %% % Edit mode
\usepackage{amsmath}
\usepackage[english]{babel}
\usepackage[utf8]{inputenc}
\usepackage{algorithm}
%\usepackage[options ]{algorithm2e}
\usepackage[noend]{algpseudocode}
\floatname{algorithm}{Procedure}
%\renewcommand{\algorithmicrequire}{\textbf{Input:}}
%\renewcommand{\algorithmicensure}{\textbf{Output:}}
\usepackage[backend=bibtex,style=alphabetic,sorting=debug]{biblatex}
\DeclareFieldFormat{labelalpha}{\thefield{entrykey}}
\DeclareFieldFormat{extraalpha}{}
\bibliography{\bibfilenames}
\newcommand{\cmt}[1]{\noindent\textcolor{lightgreen}{\underline{[#1]}}} % comment
\newcommand{\hc}[1]{\textcolor{blue}{#1}} % highlight command --> to
% know which symbol is
% defined as a command
\newenvironment{myitemize}{\begin{itemize}}{\end{itemize}}
\newcommand{\myitem}{\item}
\newcommand{\mycite}[1]{\textcolor{darkgreen}{[#1=\cite{#1}]}}

\else
% View mode
\usepackage{cite}
\bibliographystyle{IEEEbib}
\newcommand{\cmt}[1]{} % comment
\newcommand{\hc}[1]{\textcolor{black}{#1}} % highlight command -->
                                % in the edit mode, this command can
                                % be used to color symbols defined as
                                % commands 
\newenvironment{myitemize}{}{}
\newcommand{\myitem}{}
\newcommand{\mycite}[1]{\cite{#1}}
\fi %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%% END: DO NOT TOUCH REGION %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%\input{mysymbol}
\input{include}


\begin{document}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\title{Graph Tools for Inference \\on Vector-Valued Time Series}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\author{WISENET Technical Report}

%% \author{Author A. One,~\IEEEmembership{Student Member,~IEEE,} Author
%%   B. Two,~\IEEEmembership{Member,~IEEE,}\\
%% and Author C. Three,~\IEEEmembership{Fellow,~IEEE}%
%% \thanks{The work in this paper was supported by...
%%  }
%% \thanks{Author One and Author Two  are with the Dept. of ...,
%%   University of ..., City, PostalCode
%%   Country. E-mails:\{authorone,autortwo\}@university.domain.
%% Author Three is with the Dept. of ...,
%%   University of ..., City, PostalCode
%%   Country. E-mail:author.three@university.domain.}
%% \thanks{Parts of this work have been presented in Conference XYZ.}
%% }

%\markboth{IEEE TRANSACTIONS ON ... (submitted, \today)}{One, Two, and
%  Three: IEEE Journal Paper Template}
% The only time the second header will appear is for the odd numbered pages
% after the title page when using the twoside option.


\maketitle
\begin{abstract}
This document formulates several inference problems on vector-valued
time series and surveys the relevant literature. Research directions
are identified, ideas are summarized, and possible new and relevant
solutions are investigated.
\end{abstract}


%% \begin{keywords}
%% Keyword 1, keyword 2, keyword 3, keyword 4
%% \end{keywords}



%% This is a template with editing functionality. Just set the variable
%% $\backslash{}$editmode to 1 or 0. A major difference between both
%% modes is how the citations display; see e.g.  \cite{kay1},
%% \cite{patwari2008nesh,kanso2009compressed}.


\section{Conventions to write in this document}


 \cmt{General}
  \begin{itemize}
  \item Write in a clear, simple and schematic way using short
    sentences organized in bulleted lists.
  \item Notation should be consistent with the rest of this document
  \item To formulate a problem, one must write what is given and what
    is requested
  \end{itemize}

\cmt{Sec.~\ref{sec:literature}}
  \begin{itemize}
  \item Report only aspects that are
    related to the problems in Sec.~\ref{sec:problem}
  \item The following points should be clearly stated for each
    reference
    \begin{itemize}
    \item problem
      \begin{itemize}
      \item formulation
      \item motivation
      \end{itemize}      
    \item proposed solution
      \begin{itemize}
      \item high-level description highlighting the novelty
      \item relevance justification
        \begin{itemize}
        \item intuition justifying why it works better than existing
          alternatives
        \item high-level summary of validation through analytic or
          experimental methods
        \end{itemize}      
      \end{itemize}
    \item how the work can be improved (limitations?)
    \end{itemize}

  \item Feel free to add more references, e.g. those related
    references cited in the papers reviewed here

  \end{itemize}



\section{Problem formulation}
\label{sec:problem}


\cmt{motivation}The following problems are of general interest and can
be applied to address the use cases in the Petromaks project.

\cmt{Notation}
\begin{itemize}
\item\cmt{time series} $\bm F\in \rfield^{N \times T}$ is a matrix
  whose $(n,t)$-th entry $f_n[t]$ is the $t$-th time sample of the
  $n$-th time series.
  \begin{itemize}
  \item  $\{f_n[t]\}_{t=0}^{T-1}$ is the $n$-th time series, which can
    correspond to the measurements gathered by the $n$-th sensor
  \item Define $\bm f[t]\define [f_1[t],\ldots,f_{N}[t]]^T$. With
    this notation $\{\bm f[t]\}_{t=0}^{T-1}$ is called
    \emph{vector-valued time series}.
  \end{itemize}
\item\cmt{sampling} $\Omega(\bm F)$ is a subset of the entries of $\bm F$.
  \begin{itemize}
  \item  \cmt{example}$\Omega(\bm F)
    =\{f_3[4],f_{10}[5],f_{11}[8]\}$
  \item\cmt{complement} $\Omega^c(\bm F)$ is the complement of $\Omega(\bm F)$, i.e.,
    $\Omega^c(\bm F)$ contains all entries of $\bm F$ not in $\Omega(\bm F)$.
\end{itemize}
\end{itemize}



\cmt{Problems}
\begin{itemize}
\item\cmt{Reconstruction}
  \begin{itemize}
  \item \cmt{problem definition}
    \begin{itemize}
    \item \cmt{Given:} $\Omega(\bm F)$
    \item \cmt{Requested:} $\Omega^c(\bm F)$
    \end{itemize}
  \item\cmt{special cases}
    \begin{itemize}
    \item\cmt{Prediction}  $\Omega(\bm F)$ comprises the entries in the first $t$
        columns of $\bm F$
    \item\cmt{Temporal interpolation} $\Omega(\bm F)$ comprises some
      columns of $\bm F$ (motivated if one subsamples e.g. for
      compression purposes)
    \item\cmt{Spatial interpolation} $\Omega(\bm F)$ comprises most of
      the rows of $\bm F$, but only some entries of the rest of rows
      (motivated if a sensor fails at some point in time)
    \end{itemize}
  \end{itemize}
\item\cmt{Event detection}
  \begin{itemize}
  \item\cmt{problem definition}
    \begin{itemize}
    \item \cmt{Given:} $\Omega(\bm F)$, set of matrices $\mathcal{D}\subset \rfield^{N \times T}$
    \item \cmt{Requested:} Determine whether $\bm F\in \mathcal{D}$
    \end{itemize}
  \item\cmt{examples}
    \begin{itemize}
    \item $\mathcal{D}$ comprises matrices $\bm F$ whose
      entries are out of some nominal range. This can be useful e.g. to
      detect whether the temperature measured by some sensor is going
      to exceed a threshold in the future so one can prevent
      explosions. 
    \item $\mathcal{D}$ may be set so that $\bm F\in \mathcal{D}$
      means that one sensor is failing. 
    \end{itemize}
  \end{itemize}
\item\cmt{Compression/dimensionality reduction}
  \begin{itemize}
  \item \cmt{given:} training data, e.g. multiple realizations of $\bm
    F$
  \item \cmt{requested:} find a compression function
    $c:\rfield^{N\times T}\rightarrow \rfield^L$ and a decompression
    function $d:\rfield^L\rightarrow \rfield^{N\times T}$ such that
    $L$ is small and $\bm F \approx d(c(\bm F))$.
  \end{itemize}
\item\cmt{causal interactions}
  \begin{itemize}
  \item \cmt{informal problem formulation}
    \begin{itemize}
    \item \cmt{given:} $\bm F$
    \item \cmt{requested:} graph indicating dependencies between time series
    \end{itemize}
\end{itemize}
  
\end{itemize}

\cmt{Solution desiderata}Existing solutions can typically be improved
by accommodating one or more of the following features
\begin{itemize}
\item Distributed
\item Online
\item robust to outliers
\item Computationally inexpensive \ra big data
\item Reliance on more reasonable assumptions
\item Adaptive to changes
\end{itemize}





\section{Literature review}
\label{sec:literature}

\cmt{Overview}This section reviews works that are potentially useful
for solving the problems in Sec.~\ref{sec:problem}.

\cmt{list of keywords to find related references}The following are useful
keywords to find related references e.g. in Google Scholar.
\begin{itemize}
\item multidimensional signal processing \ra signals can be of more
  than 2 dimensions; typically images and video
\item multisensor signal processing
  \item multichannel signal processing \ra I did not find relevant papers
\item array signal processing \ra typically assumes phase
  synchronization
\item machine learning time series
\item (sparse) vector autoregression (group lasso)
\item Granger causality 
\end{itemize}


\subsection{Graph signal reconstruction (low priority)}
In the following references, a graph is given and the graph signal
does not evolve over time, that is, $T=1$.

\begin{itemize}
\item \cite{chen2015recovery}
      \begin{itemize}
    \item\cmt{problem}
      \begin{itemize}
      \item\cmt{formulation:}
      \item\cmt{motivation:}
      \end{itemize}      
    \item\cmt{proposed solution}
      \begin{itemize}
      \item\cmt{description:}
      \item\cmt{relevance:}
        \begin{itemize}
        \item\cmt{intuition:}
        \item\cmt{validation:}
        \end{itemize}      
      \end{itemize}
    \item\cmt{possible improvements:}
    \end{itemize}


    \item \cite{romero2016multikernel}
            \begin{itemize}
    \item\cmt{problem}
      \begin{itemize}
      \item\cmt{formulation:}
      \item\cmt{motivation:}
      \end{itemize}      
    \item\cmt{proposed solution}
      \begin{itemize}
      \item\cmt{description:}
      \item\cmt{relevance:}
        \begin{itemize}
        \item\cmt{intuition:}
        \item\cmt{validation:}
        \end{itemize}      
      \end{itemize}
    \item\cmt{possible improvements:}
    \end{itemize}

\end{itemize}

\subsection{Reconstruction of time-series on graphs (low priority)}
In these approaches, a graph is given.

\begin{itemize}
\item \cite{romero2017spacetime}
        \begin{itemize}
    \item\cmt{problem}
      \begin{itemize}
      \item\cmt{formulation:}
      \item\cmt{motivation:}
      \end{itemize}      
    \item\cmt{proposed solution}
      \begin{itemize}
      \item\cmt{description:}
      \item\cmt{relevance:}
        \begin{itemize}
        \item\cmt{intuition:}
        \item\cmt{validation:}
        \end{itemize}      
      \end{itemize}
    \item\cmt{possible improvements:}
    \end{itemize}

\end{itemize}


\subsection{Vector autoregression (VAR) (high priority)}
In these approaches, a graph is not given. 

\begin{itemize}
\item \cite{bach2004learning}
        \begin{itemize}
    \item\cmt{problem}
      \begin{itemize}
      \item\cmt{formulation:}
      \item\cmt{motivation:}
      \end{itemize}      
    \item\cmt{proposed solution}
      \begin{itemize}
      \item\cmt{description:}
      \item\cmt{relevance:}
        \begin{itemize}
        \item\cmt{intuition:}
        \item\cmt{validation:}
        \end{itemize}      
      \end{itemize}
    \item\cmt{possible improvements:}
    \end{itemize}


      \item \cite{songsiri2010selection}
              \begin{itemize}
    \item\cmt{problem}
      \begin{itemize}
      \item\cmt{formulation:}
      \item\cmt{motivation:}
      \end{itemize}      
    \item\cmt{proposed solution}
      \begin{itemize}
      \item\cmt{description:}
      \item\cmt{relevance:}
        \begin{itemize}
        \item\cmt{intuition:}
        \item\cmt{validation:}
        \end{itemize}      
      \end{itemize}
    \item\cmt{possible improvements:}
    \end{itemize}


\item \cite{bolstad2011groupsparse}
      \begin{itemize}
    \item\cmt{problem}
      \begin{itemize}
      \item\cmt{formulation:}
      \item\cmt{motivation:}
      \end{itemize}      
    \item\cmt{proposed solution}
      \begin{itemize}
      \item\cmt{description:}
      \item\cmt{relevance:}
        \begin{itemize}
        \item\cmt{intuition:}
        \item\cmt{validation:}
        \end{itemize}      
      \end{itemize}
    \item\cmt{possible improvements:}
    \end{itemize}
  
\item \cite{basu2015granger}
      \begin{itemize}
    \item\cmt{problem}
      \begin{itemize}
      \item\cmt{formulation:}
      \item\cmt{motivation:}
      \end{itemize}      
    \item\cmt{proposed solution}
      \begin{itemize}
      \item\cmt{description:}
      \item\cmt{relevance:}
        \begin{itemize}
        \item\cmt{intuition:}
        \item\cmt{validation:}
        \end{itemize}      
      \end{itemize}
    \item\cmt{possible improvements:}
    \end{itemize}

\item \cite{mei2017causal}
      \begin{itemize}
    \item\cmt{problem}
      \begin{itemize}
      \item\cmt{formulation:}
      \item\cmt{motivation:}
      \end{itemize}      
    \item\cmt{proposed solution}
      \begin{itemize}
      \item\cmt{description:}
      \item\cmt{relevance:}
        \begin{itemize}
        \item\cmt{intuition:}
        \item\cmt{validation:}
        \end{itemize}      
      \end{itemize}
    \item\cmt{possible improvements:}
    \end{itemize}
  
 
\end{itemize}

\subsection{Topology identification}

\begin{itemize}
\item \cite{granger1988causality}
      \begin{itemize}
    \item\cmt{problem}
      \begin{itemize}
      \item\cmt{formulation:}
      \item\cmt{motivation:}
      \end{itemize}      
    \item\cmt{proposed solution}
      \begin{itemize}
      \item\cmt{description:}
      \item\cmt{relevance:}
        \begin{itemize}
        \item\cmt{intuition:}
        \item\cmt{validation:}
        \end{itemize}      
      \end{itemize}
    \item\cmt{possible improvements:}
    \end{itemize}

\end{itemize}



\subsection{Manifold learning}

\section{Proposed methods}
 \begin{itemize}
	\item\cmt{Model:}\\
Multivariate autoregressive (MAR) model:
\begin{equation}\label{model}
\bm f[t]=\bm u[t]+\sum_{p=1}^{P}\mathbf A_p \bm f[t-p],~~~~~t=0,1,2,...
\end{equation}
where each entry of $\bm A_p$, i.e., $a_{n,n^{\prime}}^{(p)}$ denotes the autoregressive coefficient that describes the influence of node $n'$  on $n$ at a delay of $p$ time samples, $\bm u[t]=\left [ u_1[t],~ u_2[t],~ ..., ~u_N[t]\right ]^T \sim  \mathcal{N}(\bm 0,\Sigma)$ denotes noise and $P$ is the order of autoregressive model. The $n$-th entry of $\bm f[t]$ in \eqref{model} is 
\begin{align}
f_n[t]&=u_n[t]+\sum_{p=1}^P\sum_{n'=1}^{N}a_{n,n'}^{(p)}f_{n'}[t-p], ~~~~~~~ n=1,2, ..., N\\
&=u_n[t]+\sum_{n':(n,n')\in \mathcal S_{\text{active}}}\sum_{p=1}^{P}a_{n,n'}^{(p)}f_{n'}[t-p], ~~~~~~~ n=1,2, ..., N
\end{align}
where $\mathcal{S}_{\text{active}}$ is the set of pairs $(n,n')$ for which $a_{n,n'}(p) \neq 0$ for some $p=1,2,..., P$.
\item \cmt {Problem definition:}\\
Given: $\{\bm f[t]\}_{t=0}^{T-1}$

Required: $\{ \bm A_p\}_{p=1}^P$   
\item \cmt{Solution:}\\
\item \cmt{Batch approach:} A single-shot solution for the above problem is presented here.\\

Estimation criterion: Consider the least squared error $\mathcal{E}$ given by:
\begin{align}\label{key}
\mathcal{E}&= \sum_{\tau=P}^{T-1}\left \lVert \bm f[\tau]-\sum_{p=1}^{P} \bm A_p \bm f[\tau -p]\right \lVert_2^2\\ 
&=\sum_{n=1}^{N}\sum_{\tau=P}^{T-1}\left ( f_n[\tau]-\sum_{n'=1}^{N}\sum_{p=1}^{P} a_{n,n'}^{(p)} f_{n'}[\tau-p]\right )^2
\end{align}
 Let\\
$\bm a_{n,n'}:=[a_{n,n'}^{(1)},a_{n,n'}^{(2)},...,a_{n,n'}^{(P)}]^T ~~\in \mathbb R^ {P}$\\

$\bm a_{n}:=[\bm a_{n,1}^T,\bm a_{n,2}^T,...,\bm a_{n,N}^T]^T ~~ \in \mathbb R^ {PN}$\\
and we consider that the estimation criterion is the sum of $\mathcal{E}$ and a group sparsity-promoting term, i.e., 
\begin{align}
\{ \hat{\bm a}_n\}_{n=1}^N&=\underset{ \{\bm a_n\}_{n=1}^N}{\arg\min} ~ \sum_{n=1}^{N}\sum_{\tau=P}^{T-1}\left ( f_n[\tau]-\sum_{n'=1}^{N}\sum_{p=1}^{P} a_{n,n'}^{(p)} f_{n'}[\tau-p]\right )^2+\lambda \sum_{n=1}^{N} \sum_{n'=1}^{N} \left \lVert \bm a_{n,n'}\right\rVert_2\\ 
&=\underset{ \{\bm a_n\}_{n=1}^N}{\arg\min} ~ \sum_{n=1}^{N} \left \{\sum_{\tau=P}^{T-1}\left ( f_n[\tau]-\sum_{n'=1}^{N}\sum_{p=1}^{P} a_{n,n'}^{(p)} f_{n'}[\tau-p]\right )^2+\lambda \sum_{n'=1}^{N} \left \lVert \bm a_{n,n'}\right\rVert_2 \right \}\\
&=\underset{ \{\bm a_n\}_{n=1}^N}{\arg\min} ~ \sum_{n=1}^{N} \left \{\mathcal{E}^{(n)}(\bm a_n)+\lambda \sum_{n'=1}^{N} \left \lVert \bm a_{n,n'}\right\rVert_2 \right \},
\end{align}
where $\mathcal{E}^{(n)}(\bm a_n)$ is the least squared error for node $n$.
By using the property of separability, the optimization problem for each $\bm a_n$ can be written as
\begin{equation}
\hat{\bm a}_n=  \underset{ \bm a_n}{\arg\min} ~\mathcal{E}^{(n)}(\bm a_n)+\lambda \sum_{n'=1}^{N} \left \lVert \bm a_{n,n'}\right\rVert_2 .
\end{equation}
\\
\item \cmt{Online approach:} Now instead of using the whole data, only a window of the most current data will be used in least squared error. 
\item \cmt {Problem definition:}\\
Given: $\{\bm f[\tau]\}_{\tau =0}^{t}$

Required: $\{ \bm a_n[t]\}_{n=1}^N$   
\item \cmt{Solution:}\\
Sliding window regularized recursive least squares (RLS):
A recursive least squares  based solution is proposed here in order to solve the problem in an online fashion.\\ 
Estimation criterion:\\
Since, now the most recent $L$ observations are utilized, the least squared error for node $n$ at time $t$, i.e., $\mathcal{E}_L^{(n)}(\bm a_n[t],t)$ can be written as
\begin{align}
\mathcal{E}_L^{(n)}(\bm a_n[t],t)&=\sum_{\tau=t-L+1}^{t}\left ( f_n[\tau]-\sum_{n'=1}^{N}\sum_{p=1}^{P} a_{n,n'}^{(p)}[t] f_{n'}[\tau-p]\right )^2\\
&=\sum_{\tau=t-L+1}^{t}\left ( f_n[\tau]-\sum_{n'=1}^{N} \left [ a_{n,n'}^{(1)}[t], a_{n,n'}^{(2)}[t], \ldots, a_{n,n'}^{(P)}[t]\right]
\begin{bmatrix}
f_{n'}[\tau -1]\\
f_{n'}[\tau -2]\\
\vdots\\
f_{n'}[\tau -P]\\
\end{bmatrix}
 \right )^2. \label{eqt2}
\end{align} 
Let $\bm a_{n,n'}[t]:=[a_{n,n'}^{(1)}[t], ~ a_{n,n'}^{(2)}[t],~ ..., ~a_{n,n'}^{(P)}[t]]^T\in \mathbb R^ {P}$, thus \eqref{eqt2} is\\
\begin{align}
\mathcal{E}_L^{(n)}(\bm a_n[t],t)&=\sum_{\tau=t-L+1}^{t}\left ( f_n[\tau]-\big [ \bm a_{n,1}^T[t], \bm a_{n,2}^T[t], \ldots, \bm a_{n,N}^T[t] \big]
\begin{bmatrix}
f_{1}[\tau -1]\\
f_{1}[\tau -2]\\
\vdots\\
f_{1}[\tau -P]\\
f_{2}[\tau -1]\\
\vdots\\
f_{2}[\tau -P]\\
f_{3}[\tau -1]\\
\vdots\\
f_{N}[\tau -P]
\end{bmatrix}
\right )^2. \label{eqc}
\end{align}
Let
\begin{equation}\label{key20}
 \bm g[\tau]:=\left [f_1[\tau-1], f_1[\tau-2],\ldots ,f_1[\tau-P], f_2[\tau-1], \ldots, f_2[\tau-P], \ldots , f_N[\tau-1], \ldots , f_N[\tau-P]\right ]^T \in \mathbb R^ {NP}
\end{equation}
\begin{equation}\label{key21}
\bm a_{n}[t]:=[\bm a_{n,1}^T[t], \bm a_{n,2}^T[t],  ..., \bm a_{n,N}^T[t]]^T  \in \mathbb R^ {NP},
\end{equation}

this leads us to write the expression for $\mathcal{E}_L^{(n)}(\bm a_n[t],t)$ in the following form
\begin{equation}
\mathcal{E}_L^{(n)}(\bm a_n[t],t)=\sum_{\tau=t-L+1}^{t}\left ( f_n[\tau]- \bm g^T[\tau] \bm a_n[t] \right )^2
\end{equation}\\


The overall estimation criterion for a regularized sliding window RLS  is
\begin{align}
J(\bm a_n[t],t)= \mathcal{E}_L^{(n)}(\bm a_n[t],t)+\lambda \sum_{n'=1}^{N} \left \lVert \bm a_{n,n'}[t]\right\rVert_2. \label{a1}
\end{align}
Let $\hat{\bm a}_n^{\text{RLS}}[t]$ be a regularized RLS estimate that minimizes $J(\bm a_n[t],t)$, i.e.,
\begin{equation}\label{key}
\hat{\bm a}_n^{\text{RLS}}[t]=\underset{\bm a_n[t]}{\arg\min}~J(\bm a_n[t],t)
\end{equation}
A typical approach is to take a subgradient of the cost function of the RLS estimator in \eqref{a1}  w.r.t. $\bm a_n[t]$
\begin{align}\label{key}
\nabla_{\bm a_n[t]}^{s}J(\bm a_n[t],t) &= \frac{1}{2}\sum_{\tau=t-L+1}^{t} \nabla_{\bm a_n[t]} \left ( f_n[\tau]- \bm g^T[\tau] \bm a_n[t] \right )^2+\lambda \nabla_{\bm a_n[t]} ^s \sum_{n'=1}^{N} \left \lVert \bm a_{n,n'}[t]\right\rVert_2\\
&= \frac{1}{2}\sum_{\tau=t-L+1}^{t} \left ( -2f_n[\tau]\bm g[\tau]+2\bm g[\tau]\bm g^T[\tau]\bm a_n[t]\right) + \lambda \nabla_{\bm a_n[t]} ^s \sum_{n'=1}^{N} \left \lVert \bm a_{n,n'}[t]\right\rVert_2
\end{align}
Let us find a subgradient of the regularization term, i.e.,
\begin{align}\label{key}
\nabla_{\bm a_n[t]} ^s \sum_{n'=1}^{N}\left \lVert \bm a_{n,n'}[t]\right\rVert_2 &= \nabla_{\bm a_n[t]} ^s \big (\left \lVert \bm a_{n,1}[t]\right\rVert_2+\left \lVert \bm a_{n,2}[t]\right\rVert_2+ \ldots +\left \lVert \bm a_{n,N}[t]\right\rVert_2 \big ) \\
& =  [ \nabla_{\bm a_{n,1}[t]} ^{sT} \left \lVert \bm a_{n,1}[t]\right\rVert_2, \nabla_{\bm a_{n,2}[t]} ^{sT} \left \lVert \bm a_{n,2}[t]\right\rVert_2, \ldots, \nabla_{\bm a_{n,N}[t]} ^{sT} \left \lVert \bm a_{n,N}[t]\right\rVert_2]^T, \label{eqn19}
\end{align}
where 
\begin{equation}\label{key}
\nabla_{\bm a_{n,n'}[t]} ^s\left \lVert \bm a_{n,n'}[t]\right\rVert_2= 
\begin{cases}
\frac{\bm a_{n,n'}[t]}{\left \lVert \bm a_{n,n'}[t] \right \rVert_2} & \bm a_{n,n'}[t]\neq \bm 0\\
\bm 0 & \bm a_{n,n'}[t]= \bm 0,
\end{cases}
\end{equation}
is a valid subgradient of $\bm a_{n,n'}[t]$ with respect to $\bm a_{n,n'}[t]$.
This expression for a subgradient can be approximated as 
\begin{equation}\label{key34}
\nabla_{\bm a_{n,n'}[t]} ^s\left \lVert \bm a_{n,n'}[t]\right\rVert_2\approx \frac{\bm a_{n,n'}[t]}{\left \lVert \bm a_{n,n'}[t] \right \rVert_2+ \epsilon},
\end{equation} 
where $\epsilon$ is a small positive constant.
By setting $\nabla_{\bm a_n[t]}^{s}J(\bm a_n[t],t)=\bm 0$, it follows from \eqref{eqn19} that
\begin{align} \label{eq19}
\sum_{\tau=t-L+1}^{t}\bm g[\tau]\bm g^T[\tau]\bm a_n[t]=\sum_{\tau=t-L+1}^{t} f_n[\tau]\bm g[\tau]-\lambda \nabla_{\bm a_n[t]} ^s \sum_{n'=1}^{N} \left \lVert \bm a_{n,n'}[t]\right\rVert_2.
\end{align}
Let
\begin{equation}\label{key}
\bm \Phi [t]:=\sum_{\tau=t-L+1}^{t}\bm g[\tau]\bm g^T[\tau],
\end{equation}
and 
\begin{equation}\label{key}
\bm r[t]:=\sum_{\tau=t-L+1}^{t} f_n[\tau]\bm g[\tau],
\end{equation}
where $\bm\Phi [t]$ is the deterministic auto-correlation matrix of $\bm g[t]$ while $\bm r[t]$ is the cross-correlation between $f_n[\tau]$ and $\bm g[\tau]$. Thus, \eqref{eq19} can be written as
\begin{align} \label{eq1}
\bm\Phi [t] \bm a_n[t]=\bm r[t]-\lambda \nabla_{\bm a_n[t]} ^s \sum_{n'=1}^{N}\left \lVert \bm a_{n,n'}[t]\right\rVert_2. 
\end{align}
For simplicity, let 
\begin{equation}\label{eqgrad}
\bm h(\bm a_n[t]):=\nabla_{\bm a_n[t]} ^s \sum_{n'=1}^{N}\left \lVert \bm a_{n,n'}[t]\right\rVert_2,
\end{equation} 
then \eqref{eq1} becomes
\begin{align} \label{eq24}
\bm\Phi [t] \bm a_n[t]=\bm r[t]-\lambda  \bm h(\bm a_n[t]).
\end{align}
Let 
\begin{equation} \label{eqt}
\bm \theta [t]:=\bm r[t]-\lambda  \bm h(\bm a_n[t]),
\end{equation} then \eqref{eq24} can be written as
\begin{align} \label{eqn}
\bm\Phi [t] \bm a_n[t]=\bm \theta [t].
\end{align}
Thus a modified deterministic normal equation is obtained. The solution to \eqref{eqn}, i.e., 
\begin{equation}\label{eqsol}
\hat{\bm a}_n[t]=\bm\Phi ^{-1}[t] \bm \theta [t]
\end{equation}
 
 is the desired estimate at time $t$. In order to solve this normal equation recursively, an iterative method is used to estimate $\hat {\bm a}_n[t]$ by using the previous estimate and an update term,
\begin{equation}\label{key}
\hat {\bm a}_n[t]=\hat {\bm a}_n[t-1]+ \Delta \hat {\bm a}_n[t-1].
\end{equation}

 Recursive versions of $\bm\Phi[t]$ and $\bm r[t] $ are given by 
\begin{align}\label{eq32}
\bm\Phi [t]&=\sum_{\tau=t-L+1}^{t-1}\bm g[\tau]\bm g^T[\tau]+\bm g[t]\bm g^T[t]\\
&=\sum_{\tau=t-L}^{t-1}\bm g[\tau]\bm g^T[\tau]-\bm g[t-L]\bm g^T[t-L]+\bm g[t]\bm g^T[t]\\
&=\bm \Phi [t-1]-\bm g[t-L]\bm g^T[t-L] + \bm g[t]\bm g^T[t],
\end{align}
\begin{align}
\bm r[t]&=\sum_{\tau=t-L+1}^{t-1} f_n[\tau]\bm g[\tau]+f_n[t]\bm g[t]\\
&=\sum_{\tau=t-L}^{t-1} f_n[\tau]\bm g[\tau]-f_n[t-L]\bm g[t-L]+f_n[t]\bm g[t]\\
&=\bm r[t-1]-f_n[t-L]\bm g[t-L] + f_n[t]\bm g[t].
\end{align}


Sliding window RLS approach seems to be difficult after this step, so we switch to exponentially weighted RLS. The model remains the same here.
\item \cmt {Problem definition:}\\
Given: $\{\bm f[\tau]\}_{\tau =0}^{t}$

Required: $\{ \bm a_n[t]\}_{n=1}^N$   
\item \cmt{Exponentially weighted RLS approach:} In this approach, formulation is similar to sliding window RLS except introducing forgetting factor while considering the whole data.

Estimation criterion:\\
Since the total available data will be used in this approach, therefore, the total least squared error for all nodes at time $t$, i.e., $\mathcal{E}(t)$ is given by:
\begin{align}\label{key}
\mathcal{E}(t)&= \sum_{\tau=P}^{t} \gamma ^{t-\tau }\left \lVert \bm f[\tau]-\sum_{p=1}^{P} \bm A_p \bm f[\tau -p]\right \lVert_2^2\\ 
&=\sum_{\tau=P}^{t}\gamma ^{t-\tau }\sum_{n=1}^{N}\left ( f_n[\tau]-\sum_{n'=1}^{N}\sum_{p=1}^{P} a_{n,n'}^{(p)}[t] f_{n'}[\tau-p]\right )^2,
\end{align}
where $\gamma \in (0,1]$ is forgetting factor that assigns weights to previous error terms. We consider that the estimation criterion is the sum of $\mathcal{E}(t)$ and a group sparsity-promoting term, i.e., 
\begin{align}
\{ \hat{\bm a}_n[t]\}_{n=1}^N&=\underset{ \{\bm a_n[t]\}_{n=1}^N}{\arg\min} ~ \sum_{n=1}^{N}\sum_{\tau=P}^{t}\gamma ^{t-\tau }\left ( f_n[\tau]-\sum_{n'=1}^{N}\sum_{p=1}^{P} a_{n,n'}^{(p)}[t] f_{n'}[\tau-p]\right )^2+\lambda \sum_{n=1}^{N} \sum_{n'=1}^{N} \left \lVert \bm a_{n,n'}[t]\right\rVert_2\\ 
&=\underset{ \{\bm a_n[t]\}_{n=1}^N}{\arg\min} ~ \sum_{n=1}^{N} \left \{\sum_{\tau=P}^{t}\gamma ^{t-\tau }\left ( f_n[\tau]-\sum_{n'=1}^{N}\sum_{p=1}^{P} a_{n,n'}^{(p)}[t] f_{n'}[\tau-p]\right )^2+\lambda \sum_{n'=1}^{N} \left \lVert \bm a_{n,n'}[t]\right\rVert_2 \right \}\\
&=\underset{ \{\bm a_n[t]\}_{n=1}^N}{\arg\min} ~ \sum_{n=1}^{N} \left \{\mathcal{E}^{(n)}(\bm a_n[t],t)+\lambda \sum_{n'=1}^{N} \left \lVert \bm a_{n,n'}[t]\right\rVert_2 \right \},
\end{align}
where $\mathcal{E}^{(n)}(\bm a_n[t],t)$ is the least squared error for node $n$. Since the above problem is separable, we can write an optimization problem for each $\hat {\bm a}_n[t]$ as
\begin{equation}
\hat{\bm a}_n[t]=  \underset{ \bm a_n[t]}{\arg\min} ~\mathcal{E}^{(n)}(\bm a_n[t],t)+\lambda \sum_{n'=1}^{N} \left \lVert \bm a_{n,n'}[t]\right\rVert_2 .
\end{equation}
Let us rewrite the least squared error for a node $n$ at time $t$ by considering the definitions of $\bm a_n[t]$ and $\bm g[\tau]$ in \eqref{key20} and \eqref{key21}
\begin{equation}\label{key}
\mathcal{E}^{(n)}(\bm a_n[t],t)=\sum_{\tau=P}^{t}\gamma ^{t-\tau }\left ( f_n[\tau]- \bm g^T[\tau] \bm a_n[t] \right )^2
\end{equation} 
The overall estimation criterion for a exponentially-weighted regularized RLS is
\begin{align} \label{eqb}
J(\bm a_n[t],t)= \frac{1}{2}\sum_{\tau=P}^{t} \gamma ^{t-\tau }\left ( f_n[\tau]- \bm g^T[\tau] \bm a_n[t] \right )^2+\lambda \sum_{n'=1}^{N} \left \lVert \bm a_{n,n'}[t]\right\rVert_2. 
\end{align}
Let $\hat{\bm a}_n[t]$ be an RLS estimate at time $t$ that minimizes $J(\bm a_n[t],t)$, i.e.,
\begin{equation}\label{key}
\hat{\bm a}_n[t]=\underset{\bm a_n[t]}{\arg\min}~J(\bm a_n[t],t)
\end{equation}
Since the cost function is not differentiable, a typical approach is to take a subgradient of the cost function of the RLS estimator in \eqref{eqb}  w.r.t. $\bm a_n[t]$
\begin{align}\label{key}
\nabla_{\bm a_n[t]}^{s}J(\bm a_n[t],t) &= \frac{1}{2}\sum_{\tau=P}^{t} \nabla_{\bm a_n[t]} \gamma ^{t-\tau }\left ( f_n[\tau]- \bm g^T[\tau] \bm a_n[t] \right )^2+\lambda \nabla_{\bm a_n[t]} ^s \sum_{n'=1}^{N} \left \lVert \bm a_{n,n'}[t]\right\rVert_2\\
&= \frac{1}{2}\sum_{\tau=P}^{t} \gamma ^{t-\tau } \left ( -2f_n[\tau]\bm g[\tau]+2\bm g[\tau]\bm g^T[\tau]\bm a_n[t]\right) + \lambda\nabla_{\bm a_n[t]} ^s  \sum_{n'=1}^{N} \left \lVert \bm a_{n,n'}[t]\right\rVert_2
\end{align}
Let us find a subgradient of the regularization term, i.e.,
\begin{align}\label{key}
\nabla_{\bm a_n[t]} ^s \sum_{n'=1}^{N}\left \lVert \bm a_{n,n'}[t]\right\rVert_2 &= \nabla_{\bm a_n[t]} ^s \big (\left \lVert \bm a_{n,1}[t]\right\rVert_2+\left \lVert \bm a_{n,2}[t]\right\rVert_2+ \ldots +\left \lVert \bm a_{n,N}[t]\right\rVert_2 \big ) \\
& =  [ \nabla_{\bm a_{n,1}[t]} ^{sT} \left \lVert \bm a_{n,1}[t]\right\rVert_2, \nabla_{\bm a_{n,2}[t]} ^{sT} \left \lVert \bm a_{n,2}[t]\right\rVert_2, \ldots, \nabla_{\bm a_{n,N}[t]} ^{sT} \left \lVert \bm a_{n,N}[t]\right\rVert_2]^T,
\end{align}
where 
\begin{equation}\label{key}
\nabla_{\bm a_{n,n'}[t]} ^s\left \lVert \bm a_{n,n'}[t]\right\rVert_2= 
\begin{cases}
\frac{\bm a_{n,n'}[t]}{\left \lVert \bm a_{n,n'}[t] \right \rVert_2} & \bm a_{n,n'}[t]\neq \bm 0\\
\bm 0 & \bm a_{n,n'}[t]= \bm 0,
\end{cases}
\end{equation}
is a valid subgradient of $\left \lVert\bm a_{n,n'}[t]\right\rVert_2$ with respect to $\bm a_{n,n'}[t]$. Along the lines of \cite{eksioglu2014group}, this expression for a subgradient can be approximated as the following
\begin{equation}
\nabla_{\bm a_{n,n'}[t]} ^s\left \lVert \bm a_{n,n'}[t]\right\rVert_2\approx \frac{\bm a_{n,n'}[t]}{\left \lVert \bm a_{n,n'}[t] \right \rVert_2+ \epsilon},
\end{equation} 
where $\epsilon$ is a small positive constant.
By setting $\nabla_{\bm a_n}^{s}J(\bm a_n[t],t)=\bm 0$
\begin{align} \label{eq28}
\sum_{\tau=P}^{t}\gamma ^{t-\tau }\bm g[\tau]\bm g^T[\tau]\bm a_n[t]=\sum_{\tau=P}^{t} \gamma ^{t-\tau } f_n[\tau]\bm g[\tau]-\lambda  \sum_{n'=1}^{N} \nabla_{\bm a_n[t]} ^s\left \lVert \bm a_{n,n'}[t]\right\rVert_2.
\end{align}
Let
\begin{equation}\label{key}
\bm \Phi [t]:=\sum_{\tau=P}^{t} \gamma ^{t-\tau } \bm g[\tau]\bm g^T[\tau],
\end{equation}
and 
\begin{equation}\label{key}
\bm r[t]:=\sum_{\tau=P}^{t} \gamma ^{t-\tau } f_n[\tau]\bm g[\tau],
\end{equation}
where $\bm\Phi [t]$ is the deterministic auto-correlation matrix of $\bm g[t]$ while $\bm r[t]$ is the cross-correlation between $f_n[\tau]$ and $\bm g[\tau]$. Thus, \eqref{eq28} can be written as
\begin{align} \label{eq34}
\bm\Phi [t] \bm a_n[t]=\bm r[t]-\nabla_{\bm a_n[t]} ^s \sum_{n'=1}^{N}\left \lVert \bm a_{n,n'}[t]\right\rVert_2.
\end{align}
For simplicity, let 
\begin{equation}
\bm h(\bm a_n[t]):=\nabla_{\bm a_n[t]} ^s \sum_{n'=1}^{N}\left \lVert \bm a_{n,n'}[t]\right\rVert_2,
\end{equation} 
then \eqref{eq34} becomes
\begin{align} \label{eq29}
\bm\Phi [t] \bm a_n[t]=\bm r[t]-\lambda  \bm h(\bm a_n[t]).
\end{align}
Let 
\begin{equation} \label{eq300}
\bm \theta [t]:=\bm r[t]-\lambda  \bm h(\bm a_n[t]),
\end{equation} then \eqref{eq29} can be written as
\begin{align} \label{eq31}
\bm\Phi [t] \bm a_n[t]=\bm \theta [t].
\end{align}
Thus a modified deterministic normal equation is obtained. The solution to \eqref{eq31}, i.e., 
\begin{equation}\label{eqsol2}
\hat{\bm a}_n[t]=\bm\Phi ^{-1}[t] \bm \theta [t]
\end{equation}

is the desired estimate at time $t$. In order to solve this normal equation recursively, an iterative method is used to estimate $\hat {\bm a}_n[t]$ by using the previous estimate and an update term . We target an update of the form
\begin{equation}\label{key}
\hat {\bm a}_n[t]=\hat {\bm a}_n[t-1]+ \Delta \hat {\bm a}_n[t-1],
\end{equation}
for some $\Delta \hat {\bm a}_n[t-1]$. Recursive versions of $\bm\Phi[t]$ and $\bm r[t] $ are given by 
\begin{align}\label{eq320}
\bm\Phi [t]&=\sum_{\tau=P}^{t-1} \gamma ^{t-\tau} \bm g[\tau]\bm g^T[\tau]+\bm g[t]\bm g^T[t]\\
&=\gamma \sum_{\tau=P}^{t-1} \gamma ^{t-1-\tau} \bm g[\tau]\bm g^T[\tau]+\bm g[t]\bm g^T[t]\\
&=\gamma \bm \Phi [t-1]+\bm g[t]\bm g^T[t], \label{eqpi}
\end{align}
and 
\begin{align}
\bm r [t]&=\sum_{\tau=P}^{t-1} \gamma ^{t-\tau} f_n[\tau]\bm g[\tau]+\bm f_n[t]\bm g[t]\\
&=\gamma \sum_{\tau=P}^{t-1} \gamma ^{t-1-\tau} \bm f_n[\tau]\bm g[\tau]+\bm f_n[t]\bm g[t]\\
&=\gamma \bm r [t-1]+ f_n[t] \bm g[t]. \label{eq77}
\end{align}
Now, let us derive an expression for the recursive version of $\bm \theta [t]$. By \eqref{eq300}, we can write
\begin{equation}\label{eq79}
\bm \theta [t-1]=\bm r[t-1]-\lambda  \bm h(\bm a_n[t-1]),
\end{equation} 
and by substituting \eqref{eq77} into \eqref{eq300}, we get
\begin{equation}\label{key45}
\bm \theta [t]=\gamma \bm r [t-1]+ f_n[t] \bm g[t]- \lambda \bm h(\bm a_n[t]).
\end{equation}
Again, solving for $\bm r[t-1]$ in \eqref{eq79} and substituting the result into \eqref{key45} gives the following recursive form of $\bm \theta [t]$ 
\begin{equation}\label{key}
\bm \theta [t]=\gamma \bm \theta [t-1] + \gamma \lambda \bm h(\bm a_n[t-1]) + f_n[t] \bm g[t]- \lambda \bm h(\bm a_n[t]).
\end{equation}
Assuming that the subgradient of the estimated coefficients does not change abruptly in the subsequent time step, hence we can approximate that $\bm h(\bm a_n[t])\approx \bm h(\bm a_n[t-1])$. Thus, by incorporating this assumption we can write the above expression as
\begin{equation}\label{eq42}
\bm \theta [t]=\gamma \bm \theta [t-1] + \gamma \lambda \bm h(\bm a_n[t-1]) + f_n[t] \bm g[t]- \lambda \bm h(\bm a_n[t-1]).
\end{equation}
Thus, the recursive form for $\bm \theta[t]$ has been derived. Our goal is to find a recursive expression for $\hat{\bm a}_n[t]=\bm\Phi ^{-1}[t] \bm \theta [t]$ that updates the estimate when new data arrive. Now, for finding the recursive form of $\bm \Phi ^{-1} [t]$, by \eqref{eqpi} we can write
\begin{equation}\label{eq41}
\bm \Phi^{-1}[t]=\big (\gamma \bm \Phi [t-1]+\bm g[t]\bm g^T[t] \big ) ^{-1} 
\end{equation}
Given $\bm\Phi ^{-1}[t-1]$, the inverse of the matrix in the above expression can be easily found by the matrix inversion lemma given by
\begin{equation}\label{key}
(\bm A+\bm u \bm v^H)^{-1}=\bm A^{-1}-\frac{\bm A^{-1}\bm u \bm v^H\bm A^{-1}} {1+\bm v^H \bm A^{-1}\bm u},
\end{equation}
hence using the matrix inversion lemma, we can write 
\begin{equation}\label{key12b}
\big ( \gamma \bm \Phi [t-1]+\bm g[t]\bm g^T[t] \big )^{-1}= \gamma^{-1}\bm\Phi ^{-1}[t-1]- \frac{\gamma ^{-2}\bm\Phi ^{-1}[t-1]\bm g[t] \bm g^T[t]\bm\Phi ^{-1}[t-1]}{1+\gamma ^{-1}\bm g^T[t]\bm\Phi ^{-1}[t-1]\bm g[t]}.
\end{equation}
Let $\bm Q[t]:=\bm\Phi ^{-1}[t]$ and 
\begin{equation} \label{eq36b}
\bm k[t]:=\frac{\gamma ^{-1} \bm\Phi ^{-1} [t-1]\bm g[t]}{1+ \gamma ^{-1}\bm g^T[t]\bm\Phi ^{-1}[t-1]\bm g[t]}
\end{equation}
Thus \eqref{key12b} turns out to be
\begin{equation}\label{eq39b}
\bm\Phi ^{-1}[t]=\bm Q[t]=\gamma ^{-1} \bm Q[t-1]- \gamma ^{-1} \bm k[t]\bm g^T[t] \bm Q[t-1],
\end{equation} 
which is our desired recursive expression for $\bm\Phi ^{-1}[t]$. To get a simplified expression for $\bm k[t]$ in terms of $\bm Q[t]$, it follows from \eqref{eq36b} that
\begin{equation}\label{key}
\bm k[t]\big(1+ \gamma ^{-1}\bm g^T[t]\bm\Phi ^{-1}[t-1]\bm g[t]\big )=\gamma ^{-1} \bm\Phi ^{-1} [t-1]\bm g[t] ,
\end{equation}
this implies that
\begin{align}\label{key}
\bm k[t]&= \gamma ^{-1} \bm\Phi ^{-1} [t-1]\bm g[t]-\gamma ^{-1}\bm k[t]\bm g^T[t]\bm\Phi ^{-1}[t-1]\bm g[t]\\
&=\big ( \gamma ^{-1}  \bm Q[t-1]-\gamma^{-1}\bm k[t]\bm g^T[t]  \bm Q[t-1]\big )\bm g[t]
\end{align}
By \eqref{eq39b}, the above expression can be written as
\begin{equation}\label{key16}
\bm k[t]= \bm Q[t]\bm g[t],
\end{equation} 
this implies that $\bm k[t]$ is the solution to $ \bm \Phi [t] \bm k[t]=\bm g[t]$. To get a recursive version of $\hat{\bm a}_n[t]$, substitute \eqref{eq39b} and \eqref{eq42} in \eqref{eqsol2}
\begin{align*}\label{key}
\hat{\bm a}_n[t]&=\big ( \gamma ^{-1}  \bm Q[t-1]- \gamma ^{-1} \bm k[t]\bm g^T[t] \bm Q[t-1] \big ) \big ( \gamma \bm \theta [t-1] + \gamma \lambda \bm h(\bm a_n[t-1]) + f_n[t] \bm g[t]- \lambda \bm h(\bm a_n[t-1]) \big )\\
&= \bm Q[t-1] \bm \theta [t-1]+\lambda  \bm Q[t-1]\bm h(\bm a_n[t-1])+\gamma ^{-1} f_n[t]  \bm Q[t-1] \bm g[t]- \gamma ^{-1}\lambda \bm Q[t-1]\bm h(\bm a_n[t-1])\\
&- \bm k[t]\bm g^T[t] \bm Q[t-1] \bm \theta[t-1]- \lambda \bm k[t]\bm g^T[t] \bm Q[t-1]\bm h(\bm a_n[t-1]) - \gamma^{-1} \bm k[t]\bm g^T[t] \bm Q[t-1] f_n[t]\bm g[t]\\
&- \gamma^{-1} \lambda \bm k[t]\bm g^T[t]\bm Q[t-1]\bm h(\bm a_n[t-1])
\end{align*}
It follows from \eqref{eqsol2} that $ \hat{\bm a}_n[t-1]=\bm Q[t-1] \bm \theta [t-1]$ and by exploiting \eqref{key16}, the above expression becomes
\begin{align*}
\hat{\bm a}_n[t]&=\hat{\bm a}_n[t-1] + f_n[t]\bm k[t]-\bm k[t]\bm g^T[t]\hat{\bm a}_n[t-1] +\lambda(\frac{\gamma -1}{\gamma})\bm  Q[t-1]\bm h(\bm a_n[t-1])\\
&- \lambda(\frac{\gamma -1}{\gamma}) \bm k[t]\bm g^T[t]\bm Q[t-1]\bm h(\bm a_n[t-1]) \\
&=\hat{\bm a}_n[t-1] + \bm k[t]\big ( f_n[t]-g^T[t]\hat{\bm a}_n[t-1] \big ) + \lambda(\frac{\gamma -1}{\gamma}) \Big( \bm I- \bm k[t]\bm g^T[t] \Big)\bm  Q[t-1]\bm h(\bm a_n[t-1])\\
&=\hat{\bm a}_n[t-1] + e[t] \bm k[t] + \lambda(\frac{\gamma -1}{\gamma}) \Big( \bm I- \bm k[t]\bm g^T[t] \Big)\bm  Q[t-1]\bm h(\bm a_n[t-1]),
\end{align*}
where $e[t]:=f_n[t]-\bm g^T[t]\hat{\bm a}_n[t-1]$ is the expression for a priori error. Thus, we have derived our desired algorithm that is given in the following algorithmic form.

%\begin{algorithm}
%\caption{Exponentially weighted RLS algorithm }\label{alg:rls}
%\textbf{Input:} $P, \lambda, \gamma, \{\bm f[\tau]\}_{\tau =0}^{t} $ \\
%\textbf{Output:} $\{ \bm a_n[t]\}_{n=1}^N$ \\
%\textbf{Initialization:} $ \bm a_n[P]=0, \bm P[P]= \sigma^{-1} \bm I $
%\begin{algorithmic}[1]
%\Procedure{Euclid}{$a,b$} \Comment{The g.c.d. of a and b}
%\State $r\gets a \bmod b$
%\While{$r\not=0$} \Comment{We have the answer if r is 0}
%\State $a \gets b$
%\State $b \gets r$
%\State $r \gets a \bmod b$
%\EndWhile\label{euclidendwhile}
%\State \textbf{return} $b$\Comment{The gcd is b}
%\EndProcedure
%\end{algorithmic}
%\end{algorithm}

\begin{algorithm}
	\caption{Exponentially weighted group sparse RLS algorithm }\label{alg:rls}
	\textbf{Input:} $\sigma,P, \lambda, \gamma, \{\bm f[\tau]\}_{\tau =0}^{t} $ \Comment {$\sigma$ is a small positive number used for initializing $\bm Q$  }\\
	\textbf{Output:} $\{  \hat{\bm a}_n[t]\}_{n=1}^N$ \\
	\textbf{Initialization:} $ \hat{\bm a}_n[P-1]=\bm 0, \bm Q[P-1]= \sigma^{-1} \bm I $ \Comment {$\bm I$ has the same dimension as $\bm Q$}
	\begin{algorithmic}[1] % The number tells where the line numbering should start
		\For {$t=P,P+1, \ldots,$} \Comment {loop for time}
		\State $\displaystyle{\bm k[t]=\frac{ \bm Q [t-1]\bm g[t]}{\gamma+ \bm g^T[t]\bm  Q[t-1]\bm g[t]}}$ \Comment {computing gain vector} 
		\State $\bm  Q[t]=\gamma ^{-1} \bm  Q[t-1]- \gamma ^{-1} \bm k[t]\bm g^T[t]\bm  Q[t-1]$ \Comment {calculating $ \bm Q[t]$ for next iteration }
		\For {$n=1,2, \ldots, N$} \Comment {loop for nodes}
		
		\State $\displaystyle{e[t]= f_n[t]-\bm g^T[t]\hat{\bm a}_n[t-1]} $ \Comment { calculating a priori error}
		\State $\hat{\bm a}_n[t]=\hat{\bm a}_n[t-1] + e[t] \bm k[t] + \lambda(\gamma -1) \bm  Q[t]\bm h(\bm a_n[t-1])$ \Comment {updating the estimate}
		
		\EndFor
		\State \textbf{end for}
		\EndFor
		\State \textbf{end for}
	\end{algorithmic}
\end{algorithm}
\end{itemize}

%%%%%%%%%%%%% DO NOT MODIFY  %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\if\editmode1 
\onecolumn
\printbibliography
\else
\bibliography{\bibfilenames}
\fi
\end{document}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


